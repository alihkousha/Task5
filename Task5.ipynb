{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f0d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk.corpus\n",
    "from unidecode                        import unidecode\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import SnowballStemmer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8df708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed libraries for previous build\n",
    "from nltk.stem import PorterStemmer\n",
    "import webcolors\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba35c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Path = os.path.join(os.getcwd(),'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474c278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(Data_Path,'Amazon Search Terms_Search Terms_US.csv')).sample(frac=0.01)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42780b9a",
   "metadata": {},
   "source": [
    "# Optimized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62403f8",
   "metadata": {},
   "source": [
    "## Data PreProcess New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d640595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWords(listOfTokens, listOfWords):\n",
    "    return [token for token in listOfTokens if token not in listOfWords]\n",
    "\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n",
    "\n",
    "def twoLetters(listOfTokens):\n",
    "    twoLetterWord = []\n",
    "    for token in listOfTokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            twoLetterWord.append(token)\n",
    "    return twoLetterWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9131093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(Dataset):   \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    param_stemmer = SnowballStemmer('english')\n",
    "    other_words = [line.rstrip('\\n') for line in open('lists/stopwords_scrapmaker.txt')] # Load .txt file line by line\n",
    "    \n",
    "    for document in Dataset:\n",
    "        try:\n",
    "            index = Dataset.index(document)\n",
    "            Dataset[index] = Dataset[index].replace(u'\\ufffd', '8')   # Replaces the ASCII 'ï¿½' symbol with '8'\n",
    "            Dataset[index] = Dataset[index].replace(',', '')          # Removes commas\n",
    "            Dataset[index] = Dataset[index].rstrip('\\n')              # Removes line breaks\n",
    "            Dataset[index] = Dataset[index].casefold()                # Makes all letters lowercase\n",
    "            \n",
    "            Dataset[index] = re.sub('\\W_',' ', Dataset[index])        # removes specials characters and leaves only words\n",
    "            Dataset[index] = re.sub(\"\\S*\\d\\S*\",\" \", Dataset[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
    "            Dataset[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", Dataset[index]) # removes emails and mentions (words with @)\n",
    "            Dataset[index] = re.sub(r'http\\S+', '', Dataset[index])   # removes URLs with http\n",
    "            Dataset[index] = re.sub(r'www\\S+', '', Dataset[index])    # removes URLs with www\n",
    "            Dataset[index] = re.sub(r'www\\S+', '', Dataset[index])    # removes URLs with www\n",
    "        except:\n",
    "            print(index)\n",
    "        listOfTokens = word_tokenize(Dataset[index])\n",
    "        twoLetterWord = twoLetters(listOfTokens)\n",
    "\n",
    "        listOfTokens = removeWords(listOfTokens, stopwords)\n",
    "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "        \n",
    "        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "\n",
    "        Dataset[index]   = \" \".join(listOfTokens)\n",
    "        Dataset[index] = unidecode(Dataset[index])\n",
    "\n",
    "    return Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1a54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "products =  []\n",
    "zipped = zip(df['#1 Product Title'].to_list() , df['#2 Product Title'].to_list() , df['#3 Product Title'].to_list())\n",
    "for item in zipped:\n",
    "    x,y,z = item[0],item[1],item[2]\n",
    "    products.append(x)\n",
    "    products.append(y)\n",
    "    products.append(z)\n",
    "products_raw = products.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5ab04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [ele for ele in products if ele is not None]\n",
    "products = processData(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293a33d",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211ddd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(products)\n",
    "tf_idf_X = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ca479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPtimized_Search_system(KNN_model: NearestNeighbors,Search_String : list,products = products):\n",
    "    Search_q = processData(Search_String)\n",
    "    Search_vectorized = vectorizer.transform(Search_q)\n",
    "    NNs = KNN_model.kneighbors(Search_vectorized, return_distance=True)\n",
    "    top = NNs[1][0][1:]\n",
    "    recommendation = pd.DataFrame(columns = ['Search Term',  'Results', 'score'])\n",
    "    count = 0\n",
    "    index_score = NNs[0][0][1:]\n",
    "    for i in top:\n",
    "        recommendation.at[count, 'Search Term'] = Search_String[0]\n",
    "        recommendation.at[count, 'Results'] = products[i]\n",
    "        recommendation.at[count, 'score'] =  index_score[count]\n",
    "        count += 1\n",
    "    recommendation.index +=1\n",
    "    return recommendation.loc[:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c49e618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = NearestNeighbors(\n",
    "    n_neighbors= 6,\n",
    "    )\n",
    "KNN.fit(tf_idf_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda4e11",
   "metadata": {},
   "source": [
    "# Previous Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867834d",
   "metadata": {},
   "source": [
    "## Previous PreProcess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3663c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemSentence(sentence):\n",
    "    porter = PorterStemmer()\n",
    "    token_words = word_tokenize(sentence)\n",
    "    stem_sentence = [porter.stem(word) for word in token_words]\n",
    "    return ' '.join(stem_sentence)\n",
    "def Clean_Sequence(X=X):\n",
    "    product = X\n",
    "    product = [remove_stopwords(str(x))\\\n",
    "            .translate(str.maketrans('','',string.punctuation))\\\n",
    "            .translate(str.maketrans('','',string.digits))\\\n",
    "            for x in products]\n",
    "    \n",
    "    product = pd.Series([stemSentence(str(x)) for x in products])    \n",
    "    colors = list(webcolors.CSS3_NAMES_TO_HEX)\n",
    "    colors = [stemSentence(str(x)) for x in colors if x not in ('bisque','blanchedalmond','chocolate','honeydew','lime',\n",
    "                                             'olive','orange','plum','salmon','tomato','wheat')]\n",
    "\n",
    "    \n",
    "    product = [' '.join([x for x in str(string).split() if x not in colors]) for string in products]\n",
    "\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e71266",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_2 = Clean_Sequence(products_raw.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b539d0f",
   "metadata": {},
   "source": [
    "## Previous Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bdd0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_productid = tfidf_vectorizer.fit_transform(products_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef48dec",
   "metadata": {},
   "source": [
    "## Search Engine Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4569e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search_system(Search_String):\n",
    "    Search_q = Clean_Sequence([Search_String])\n",
    "    Search_tfidf = tfidf_vectorizer.transform(Search_q)\n",
    "    return [Search_tfidf,Search_String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "164662c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(top, df_all, scores, Search_String):\n",
    "    recommendation = pd.DataFrame(columns = ['Search Term',  'Results Previous Model', 'score_2'])\n",
    "    count = 0\n",
    "    for i in top:\n",
    "        recommendation.at[count, 'Search Term'] = Search_String\n",
    "        recommendation.at[count, 'Results Previous Model'] = df_all[i]\n",
    "        recommendation.at[count, 'score_2'] =  scores[count]\n",
    "        count += 1\n",
    "    recommendation.index += 1\n",
    "    return recommendation.loc[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c09c3413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_2 = NearestNeighbors(n_neighbors = 6)\n",
    "KNN_2.fit(tfidf_productid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4b5ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_model_Search(ST:str):\n",
    "    Searched = Search_system(ST)\n",
    "    Search_tfidf,Search_string = Searched[0],Searched[1]\n",
    "    NNs = KNN_2.kneighbors(Search_tfidf, return_distance=True)\n",
    "    top = NNs[1][0][1:]\n",
    "    index_score = NNs[0][0][1:]\n",
    "    return get_recommendation(top, products_raw, index_score, Search_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f8d85",
   "metadata": {},
   "source": [
    "# Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eeebd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Search Term</th>\n",
       "      <th>Results</th>\n",
       "      <th>score</th>\n",
       "      <th>Results Previous Model</th>\n",
       "      <th>score_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>After: The After Series, Book 1</td>\n",
       "      <td>0.743758</td>\n",
       "      <td>5 LB - Ultra Clear Glycerin Soap Base by velon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book</td>\n",
       "      <td>Before (The After Series Book 5)</td>\n",
       "      <td>0.743758</td>\n",
       "      <td>Pifito Clear Melt and Pour Soap Base (2 lb) â ...</td>\n",
       "      <td>0.874603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>The Quiet Book padded board book</td>\n",
       "      <td>0.75322</td>\n",
       "      <td>NF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book</td>\n",
       "      <td>The Ultimate Bar Book: The Comprehensive Guide...</td>\n",
       "      <td>0.76426</td>\n",
       "      <td>Blue on Black</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>book</td>\n",
       "      <td>The Comfort Book</td>\n",
       "      <td>0.778027</td>\n",
       "      <td>Near You</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Search Term                                            Results     score  \\\n",
       "1        book                    After: The After Series, Book 1  0.743758   \n",
       "2        book                   Before (The After Series Book 5)  0.743758   \n",
       "3        book                   The Quiet Book padded board book   0.75322   \n",
       "4        book  The Ultimate Bar Book: The Comprehensive Guide...   0.76426   \n",
       "5        book                                   The Comfort Book  0.778027   \n",
       "\n",
       "                              Results Previous Model   score_2  \n",
       "1  5 LB - Ultra Clear Glycerin Soap Base by velon...       0.0  \n",
       "2  Pifito Clear Melt and Pour Soap Base (2 lb) â ...  0.874603  \n",
       "3                                                 NF       1.0  \n",
       "4                                      Blue on Black       1.0  \n",
       "5                                           Near You       1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Search_input = input('what do you want to search for?')\n",
    "final_df_1 = OPtimized_Search_system(\n",
    "    KNN_model = KNN,\n",
    "    Search_String=[Search_input],\n",
    "    products= products_raw\n",
    ")\n",
    "final_df_2 = previous_model_Search(Search_input)\n",
    "pd.concat([final_df_1,final_df_2['Results Previous Model'],final_df_2['score_2']],axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
